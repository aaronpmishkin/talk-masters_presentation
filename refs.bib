%% bibliography %%

 
    ### Growth Conditions ###

%% Strong Growth + Noise
@article{poljak1973pseudogradient,
  title={Pseudogradient adaptation and training algorithms},
  author={Polyak, BT and Tsypkin, Ya Z},
  journal={Automation and Remote Control},
  volume={34},
  pages={45--67},
  year={1973}
}

%% Maximal Strong Growth 
@article{tseng1998incremental,
  author    = {Paul Tseng},
  title     = {An Incremental Gradient(-Projection) Method with Momentum Term and
               Adaptive Stepsize Rule},
  journal   = {{SIAM} Journal on Optimization},
  volume    = {8},
  number    = {2},
  pages     = {506--531},
  year      = {1998}
}
%% Maximal Strong Growth 
@article{solodov1998incremental,
  author    = {Mikhail V. Solodov},
  title     = {Incremental Gradient Algorithms with Stepsizes Bounded Away from Zero},
  journal   = {Comp. Opt. and Appl.},
  volume    = {11},
  number    = {1},
  pages     = {23--35},
  year      = {1998}
}

%% Strong Growth 
@article{schmidt2013fast,
  title     = {Fast convergence of stochastic gradient descent under a strong growth condition},
  author    = {Schmidt, Mark and {Le Roux}, Nicolas},
  journal   = {arXiv preprint arXiv:1308.6370},
  year      = {2013}
}


%% Strong Growth is necessary for linear convergence; alternative weak growth condition (SGC + noise);
%% convergence to neighbourhood for (proximal-) gradient descent under WGC + noise.
@article{cevher2018linear,
  author    = {Volkan Cevher and
               Bang C{\^{o}}ng Vu},
  title     = {On the linear convergence of the stochastic gradient method with constant
               step-size},
  journal   = {Optim. Lett.},
  volume    = {13},
  number    = {5},
  pages     = {1177--1187},
  year      = {2019},
}


%% Strong Growth + Weak Growth + Noise
@article{khaled2020better,
  title     = {Better Theory for {SGD} in the Nonconvex World},
  author    = {Khaled, Ahmed and Richt{\'a}rik, Peter},
  journal   = {arXiv preprint arXiv:2002.03329},
  year      = {2020}
}


    ### Interpolation ###

# Belkin's Group #

@inproceedings{ma2018power,
  author    = {Siyuan Ma and
               Raef Bassily and
               Mikhail Belkin},
  _editor    = {Jennifer G. Dy and
               Andreas Krause},
  title     = {The Power of Interpolation: Understanding the Effectiveness of {SGD}
               in Modern Over-parametrized Learning},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning,
               {ICML} 2018},
  series    = {Proceedings of Machine Learning Research},
  volume    = {80},
  pages     = {3331--3340},
  publisher = {{PMLR}},
  year      = {2018},
}

@article{bassily2018exponential,
  title     = {On exponential convergence of {SGD} in non-convex over-parametrized learning},
  author    = {Bassily, Raef and Belkin, Mikhail and Ma, Siyuan},
  journal   = {arXiv preprint arXiv:1811.02564},
  year      = {2018}
}

@inproceedings{liu2020accelerating,
  author    = {Chaoyue Liu and
               Mikhail Belkin},
  title     = {Accelerating {SGD} with momentum for over-parameterized learning},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020},
  publisher = {OpenReview.net},
  year      = {2020},
}

# Oxford Group #
% Bounded stochastic Polyak step-size
@article{berrada2019training,
  title     = {Training neural networks for and by interpolation},
  author    = {Berrada, Leonard and Zisserman, Andrew and Kumar, M Pawan},
  journal   = {arXiv preprint arXiv:1906.05661},
  year      = {2019}
}

# UBC Group #

@inproceedings{vaswani2019fast,
  author    = {Sharan Vaswani and
               Francis Bach and
               Mark W. Schmidt},
  _editor    = {Kamalika Chaudhuri and
               Masashi Sugiyama},
  title     = {Fast and Faster Convergence of {SGD} for Over-Parameterized Models
               and an Accelerated Perceptron},
  booktitle = {The 22nd International Conference on Artificial Intelligence and Statistics,
               {AISTATS} 2019},
  series    = {Proceedings of Machine Learning Research},
  volume    = {89},
  pages     = {1195--1204},
  publisher = {{PMLR}},
  year      = {2019}
}


@inproceedings{vaswani2019painless,
  author    = {Sharan Vaswani and
               Aaron Mishkin and
               Issam H. Laradji and
               Mark Schmidt and
               Gauthier Gidel and
               Simon Lacoste{-}Julien},
  _editor    = {Hanna M. Wallach and
               Hugo Larochelle and
               Alina Beygelzimer and
               Florence d'Alch{\'{e}}{-}Buc and
               Emily B. Fox and
               Roman Garnett},
  title     = {Painless Stochastic Gradient: Interpolation, Line-Search, and Convergence
               Rates},
  booktitle = {Advances in Neural Information Processing Systems 32: {NeurIPS} 2019},
  pages     = {3727--3740},
  year      = {2019},
}

@inproceedings{meng2020fastandfurious,
  author    = {Si Yi Meng and
               Sharan Vaswani and
               Issam Hadj Laradji and
               Mark Schmidt and
               Simon Lacoste{-}Julien},
  _editor    = {Silvia Chiappa and
               Roberto Calandra},
  title     = {Fast and Furious Convergence: Stochastic Second Order Methods under
               Interpolation},
  booktitle = {The 23rd International Conference on Artificial Intelligence and Statistics,
               {AISTATS} 2020},
  series    = {Proceedings of Machine Learning Research},
  volume    = {108},
  pages     = {1375--1386},
  publisher = {{PMLR}},
  year      = {2020},
}
 
@article{loizou2020sps,
  title     = {Stochastic {P}olyak step-size for {SGD}: An adaptive learning rate for fast convergence},
  author    = {Loizou, Nicolas and Vaswani, Sharan and Laradji, Issam and Lacoste-Julien, Simon},
  journal   = {arXiv preprint arXiv:2002.10542},
  year      = {2020}
}

@article{vaswani2020adaptive,
  title     = {Adaptive Gradient Methods Converge Faster with Over-Parameterization (and you can do a line-search)},
  author    = {Vaswani, Sharan and Kunstner, Frederik and Laradji, Issam and Meng, Si Yi and Schmidt, Mark and Lacoste-Julien, Simon},
  journal   = {arXiv preprint arXiv:2006.06835},
  year      = {2020}
}

# Other Affiliations #

@article{wu2019global,
  title={Global convergence of adaptive gradient methods for an over-parameterized neural network},
  author={Wu, Xiaoxia and Du, Simon S and Ward, Rachel},
  journal={arXiv preprint arXiv:1902.07111},
  year={2019}
}

    ### Step-Size Selection ###

# Armijo Line-Search #

%% original Armijo line-search reference
@article{armijo1966ls,
  author      = {Armijo, Larry},
  journal     = {Pacific Journal of Mathematics},
  number      = {1},
  pages       = {1--3},
  publisher   = {Pacific Journal of Mathematics, A Non-profit Corporation},
  title       = {Minimization of functions having {L}ipschitz continuous first partial derivatives.},
  volume      = {16},
  year        = {1966}
}


% Workhorse of Machine Learning Papers: 16 so far...

@article{xu2017second,
  title={Second-order optimization for non-convex machine learning: An empirical study},
  author={Xu, Peng and Roosta-Khorasani, Farbod and Mahoney, Michael W},
  journal={arXiv preprint arXiv:1708.07827},
  year={2017}
}

@article{zhang2016parallel,
  title={Parallel {SGD}: {W}hen does averaging help?},
  author={Zhang, Jian and De Sa, Christopher and Mitliagkas, Ioannis and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:1606.07365},
  year={2016}
}

@book{patterson2017deep,
  title={Deep learning: A practitioner's approach},
  author={Patterson, Josh and Gibson, Adam},
  year={2017},
  publisher={" O'Reilly Media, Inc."}
}

@inproceedings{pillaud2018statistical,
  title={Statistical optimality of stochastic gradient descent on hard learning problems through multiple passes},
  author={Pillaud-Vivien, Loucas and Rudi, Alessandro and Bach, Francis},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8114--8124},
  year={2018}
}

@inproceedings{grosse2015scaling,
  title={Scaling up natural gradient by sparsely factorizing the inverse fisher matrix},
  author={Grosse, Roger and Salakhudinov, Ruslan},
  booktitle={International Conference on Machine Learning},
  pages={2304--2313},
  year={2015}
}

@article{assran2018stochastic,
  title={Stochastic gradient push for distributed deep learning},
  author={Assran, Mahmoud and Loizou, Nicolas and Ballas, Nicolas and Rabbat, Michael},
  journal={arXiv preprint arXiv:1811.10792},
  year={2018}
}

@inproceedings{damaskinos2019aggregathor,
  title={Aggregathor: Byzantine machine learning via robust gradient aggregation},
  author={Damaskinos, Georgios and El Mhamdi, El Mahdi and Guerraoui, Rachid and Guirguis, Arsany Hany Abdelmessih and Rouault, S{\'e}bastien Louis Alexandre},
  booktitle={The Conference on Systems and Machine Learning (SysML), 2019},
  number={CONF},
  year={2019}
}

@inproceedings{kawaguchi2020ordered,
  title={Ordered {SGD}: {A} New Stochastic Optimization Framework for Empirical Risk Minimization},
  author={Kawaguchi, Kenji and Lu, Haihao},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={669--679},
  year={2020}
}

@article{bernstein2018signsgd,
  title={{signSGD} with majority vote is communication efficient and fault tolerant},
  author={Bernstein, Jeremy and Zhao, Jiawei and Azizzadenesheli, Kamyar and Anandkumar, Anima},
  journal={arXiv preprint arXiv:1810.05291},
  year={2018}
}

@inproceedings{li2019rsa,
  title={{RSA}: {B}yzantine-robust stochastic aggregation methods for distributed learning from heterogeneous datasets},
  author={Li, Liping and Xu, Wei and Chen, Tianyi and Giannakis, Georgios B and Ling, Qing},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={1544--1551},
  year={2019}
}

@article{agarwal2017second,
  title={Second-order stochastic optimization for machine learning in linear time},
  author={Agarwal, Naman and Bullins, Brian and Hazan, Elad},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={4148--4187},
  year={2017},
  publisher={JMLR. org}
}

@inproceedings{hofmann2015variance,
  title={Variance reduced stochastic gradient descent with neighbors},
  author={Hofmann, Thomas and Lucchi, Aurelien and Lacoste-Julien, Simon and McWilliams, Brian},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2305--2313},
  year={2015}
}

@article{geffner2019rule,
  title={A Rule for Gradient Estimator Selection, with an Application to Variational Inference},
  author={Geffner, Tomas and Domke, Justin},
  journal={arXiv preprint arXiv:1911.01894},
  year={2019}
}

@article{assran2020convergence,
  title={On the Convergence of Nesterov's Accelerated Gradient Method in Stochastic Settings},
  author={Assran, Mahmoud and Rabbat, Michael},
  journal={arXiv preprint arXiv:2002.12414},
  year={2020}
}

@article{drori2019complexity,
  title={The complexity of finding stationary points with stochastic gradient descent},
  author={Drori, Yoel and Shamir, Ohad},
  journal={arXiv preprint arXiv:1910.01845},
  year={2019}
}

@article{gower2019sgd,
  title={{SGD}: {G}eneral analysis and improved rates},
  author={Gower, Robert Mansel and Loizou, Nicolas and Qian, Xun and Sailanbayev, Alibek and Shulgin, Egor and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:1901.09401},
  year={2019}
}
