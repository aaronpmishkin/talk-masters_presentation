%% bibliography %%




% Workhorse of Machine Learning Papers: 16 so far...

@article{xu2017second,
  title={Second-order optimization for non-convex machine learning: An empirical study},
  author={Xu, Peng and Roosta-Khorasani, Farbod and Mahoney, Michael W},
  journal={arXiv preprint arXiv:1708.07827},
  year={2017}
}

@article{zhang2016parallel,
  title={Parallel {SGD}: {W}hen does averaging help?},
  author={Zhang, Jian and De Sa, Christopher and Mitliagkas, Ioannis and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:1606.07365},
  year={2016}
}

@book{patterson2017deep,
  title={Deep learning: A practitioner's approach},
  author={Patterson, Josh and Gibson, Adam},
  year={2017},
  publisher={" O'Reilly Media, Inc."}
}

@inproceedings{pillaud2018statistical,
  title={Statistical optimality of stochastic gradient descent on hard learning problems through multiple passes},
  author={Pillaud-Vivien, Loucas and Rudi, Alessandro and Bach, Francis},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8114--8124},
  year={2018}
}

@inproceedings{grosse2015scaling,
  title={Scaling up natural gradient by sparsely factorizing the inverse fisher matrix},
  author={Grosse, Roger and Salakhudinov, Ruslan},
  booktitle={International Conference on Machine Learning},
  pages={2304--2313},
  year={2015}
}

@article{assran2018stochastic,
  title={Stochastic gradient push for distributed deep learning},
  author={Assran, Mahmoud and Loizou, Nicolas and Ballas, Nicolas and Rabbat, Michael},
  journal={arXiv preprint arXiv:1811.10792},
  year={2018}
}

@inproceedings{damaskinos2019aggregathor,
  title={Aggregathor: Byzantine machine learning via robust gradient aggregation},
  author={Damaskinos, Georgios and El Mhamdi, El Mahdi and Guerraoui, Rachid and Guirguis, Arsany Hany Abdelmessih and Rouault, S{\'e}bastien Louis Alexandre},
  booktitle={The Conference on Systems and Machine Learning (SysML), 2019},
  number={CONF},
  year={2019}
}

@inproceedings{kawaguchi2020ordered,
  title={Ordered {SGD}: {A} New Stochastic Optimization Framework for Empirical Risk Minimization},
  author={Kawaguchi, Kenji and Lu, Haihao},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={669--679},
  year={2020}
}

@article{bernstein2018signsgd,
  title={{signSGD} with majority vote is communication efficient and fault tolerant},
  author={Bernstein, Jeremy and Zhao, Jiawei and Azizzadenesheli, Kamyar and Anandkumar, Anima},
  journal={arXiv preprint arXiv:1810.05291},
  year={2018}
}

@inproceedings{li2019rsa,
  title={{RSA}: {B}yzantine-robust stochastic aggregation methods for distributed learning from heterogeneous datasets},
  author={Li, Liping and Xu, Wei and Chen, Tianyi and Giannakis, Georgios B and Ling, Qing},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={1544--1551},
  year={2019}
}

@article{agarwal2017second,
  title={Second-order stochastic optimization for machine learning in linear time},
  author={Agarwal, Naman and Bullins, Brian and Hazan, Elad},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={4148--4187},
  year={2017},
  publisher={JMLR. org}
}

@inproceedings{hofmann2015variance,
  title={Variance reduced stochastic gradient descent with neighbors},
  author={Hofmann, Thomas and Lucchi, Aurelien and Lacoste-Julien, Simon and McWilliams, Brian},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2305--2313},
  year={2015}
}

@article{geffner2019rule,
  title={A Rule for Gradient Estimator Selection, with an Application to Variational Inference},
  author={Geffner, Tomas and Domke, Justin},
  journal={arXiv preprint arXiv:1911.01894},
  year={2019}
}

@article{assran2020convergence,
  title={On the Convergence of Nesterov's Accelerated Gradient Method in Stochastic Settings},
  author={Assran, Mahmoud and Rabbat, Michael},
  journal={arXiv preprint arXiv:2002.12414},
  year={2020}
}

@article{drori2019complexity,
  title={The complexity of finding stationary points with stochastic gradient descent},
  author={Drori, Yoel and Shamir, Ohad},
  journal={arXiv preprint arXiv:1910.01845},
  year={2019}
}

@article{gower2019sgd,
  title={{SGD}: {G}eneral analysis and improved rates},
  author={Gower, Robert Mansel and Loizou, Nicolas and Qian, Xun and Sailanbayev, Alibek and Shulgin, Egor and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:1901.09401},
  year={2019}
}
